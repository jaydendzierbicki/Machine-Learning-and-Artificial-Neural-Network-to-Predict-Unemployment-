# Machine Learning and Artificial Neural Network to Predict Unemployment

## Abstract/Executive Summary
The Australian economy has undergone various macroeconomic events in the past 21 years, impacting the unemployment rate, which is a crucial metric used to guide policy decisions and monitored by the ABS. This study examines the potential of machine learning and neural networks to predict and model unemployment by incorporating macroeconomic indicators, such GDP, terms of trade, CPI, job vacancies, ERP, general government consumption expenditure, and all sectors consumption expenditure. To assess performance, the root mean squared error was used as a metric, and residual plots were examined to compare the performance of these models against a basic naive model. The multivariate adaptive spline regression model (MARS) demonstrated superior performance compared to random forest and boosting models. However, the test RSME was slightly higher than the cross-validation RSME, likely due to modelling the COVID-19 pandemic's impact on unemployment. In addition, a basic neural network with two hidden layers outperformed the MARS model with a lower test RSME; though interestingly had a worse cross validation RMSE. Through increasing the complexity of our neural network to include more neurons we were able to reduce the cross validation RMSE, though MARS still outperformed in this respect. The predictive capability of these models can be utilized to predict unemployment during the waiting period for ABS unemployment reports to be published. However, it is essential to note that unpredictable events such as natural disasters, as observed during COVID-19, could impact the models' predictive power, as they are only as reliable as the training data.

## Introduction - Australia Unemployment: 1999 and 2020
The Australian Bureau of Statistics (ABS) is Australia’s national statistical agency which is responsible for collecting employment statistic such as the unemployment rate, one of many employment statistics. The unemployment rate is a key indicator of the labour market performance, providing a snapshot of the available labour supply at a particular time. The survey is conducted on about 26,000 dwellings, with responses from around 52,000 people every month, ensuring it forms a representative sample of the Australia population, with respondents being asked self-guided questions (How the ABS Measures Unemployment, 2022). There are various limitations of the unemployment rate, with the most obvious being underemployment, with the true unemployment rate tending to be larger than the actual unemployment rate. 
The Australian government often relies on the unemployment rate to shape policy decisions, especially during election cycles (Cockburn et al., 2022; Visentin et al., 2022). High unemployment rates have even been associated with election outcomes (Leigh & McLeish, 2009), which can lead to a change in government. Over the past 21 years, the Australian labour market has experienced various macroeconomic events that have influenced the unemployment rate, which has averaged around 5.6% during this period. After the recession of the 1990s, the labour market started to recover in the early 2000s, with the unemployment rate dropping to around 4.1%. However, the global financial crisis (GFC) of 2007-2008 had a significant impact on the Australian economy, causing the unemployment rate to rise to 5.75% (RBA, 2010). During such economic events, the unemployment rate is often linked to various macroeconomic indicators, such as GDP, with declining GDP leading to an increase in unemployment, as seen during recessions or the GFC (Higgens, 2011). The Covid-19 pandemic in 2019-2020 caused a sharp increase in unemployment to 7.1%, but unlike traditional recessions, the movement of other indicators such as GDP and house prices did not follow a typical recession pattern (Owen, 2022). This paper aims to predict unemployment using various macroeconomic indicators such as GDP, terms of trade, CPI, job vacancies, ERP, general government consumption expenditure, and all sectors consumption expenditure, by training two models on data from 1981 to 2018. A grid search approachwas used to tune the hyperparameters of the models. However, we acknowledge that the models may perform poorly during events such as the Covid-19 pandemic, which deviates from normal economic conditions. Therefore, the ABS survey will still be valuable and necessary for measuring unemployment during such events.

## Comparison and Suggestion
There was an obvious winner in respect to processing time, that being the MARS model which only took 61 seconds to undertake a grid search approach across the various hyper-parameters, whilst the best NN model took a total of almost 3 hours when we expended the number of neurons in the hidden layers. It was also evident from the training residual plots that the NN model was heavily overfitting; and lacked randomness in the residual plot which we did not observe as much with the MARS model. In addition, the CV accuracy of MARS was much lower than NN, 0.32 vs 1.10 respectively; this suggests that the MARS model performed better in relation to unseen data, though the NN model performed better on our validation data with a lower RMSE of 0.27 vs MARS 0.64. We did raise concerns that the validation data set was simply not large enough as it did not represent an 80:20 split which is common in ML. The interpretation of the models is vastly different; we can extract hinge functions & variable importance with ease in the MARS model, though the NN model would require additional data steps to derive figures such as variable importance which are just not as straight forward; the NN is a much black box approach. Though with NN we observe the weights associated between the layers such as the input layer and first hidden layer; we observed year tending to have a large absolute weight which represents a strong connection between the input and first hidden layer, though beyond the first hidden layer it becomes more complex with 3470 weights calculated in total. One thing we noted in our study was the use of lagged variables to predict unemployment which yielded improved RMSE in the MARS model, decreasing the optimised RMSE from 0.64 to 0.60; whilst a relatively small reduction we attempted to see if this effect translated into the NN model with improvements. The use of lagged variables in NN has been reported to lead to reduction in RMSE (Surakhi et al., 2021) though in our case resulted in a slight increase in RMSE once optimised resulting in validation RMSE of 0.354 vs 0.27; suggesting it made our NN model worse. This highlights that developing a ML algorithm is complex and requires significant resources both time and computationally, especially if the goal is to derive a well performing product as opposed to a sub-optimal product. When attempting to refine the model we observed that the gains at times can only be marginally improved, which depending on the context might no be worth the time to investigate. In addition, attempting to refine the model can also yield worse results highlighting the complex nature of machine learning which we observed with the use of lagged variables in NN; just because it worked for one model does not mean it will improve all models as we seen when we removed year. If time persisted and keras worked without API issues we would attempt the keras package to overcome some of the limitations of the NN model we produced, such as exploring various other hyper-parameters which keras offer. Though, we found through increasing the complexity of our NN model by the inclusion of more neurons that we obtained an improved validation RMSE and increased our train RMSE suggesting an improvement to overfitting which was validated by a decrease in CV RMSE for our NN when we attempted to increase the number of neurons. Overall, the NN is a much more complex black box model relative to our MARS model. 
## Conclusion & Lessons learned
Overall, both the MARS and NN models outperformed our naïve model RMSE in relation to the validation data, but we had concerns about overfitting and the insufficient size of our validation data sample to generalize about the models' performance. We observed evidence of overfitting, particularly in the NN model, which was apparent in the training residual plots. One limitation we encountered was the impact of the COVID-19 pandemic on our macroeconomic indicators. This event caused deviations from the usual trends seen during a recession, such as high unemployment and low GDP; whilst GDP dropped to -7% during Covid-19 it was quickly followed by 3% growth with high levels of unemployment. Therefore, the use of surveys to measure unemployment and other factors is crucial; and the limitations of any machine learning model must be considered. The accuracy of any machine learning model is only as good as the training data, and not all events can be captured in the training data as we live in a dynamic world. This means models will need to be periodically updated and assessed to ensure relevance, or different models employed based on different events, or the use of caveats around models developed. Furthermore, we demonstrated throughout the comparison and suggestion sections that feature engineering and selection can lead to improved results. No machine learning model is perfect, and a combination of various models or approaches may be necessary to achieve the best results for a particular problem; we observed previously through feature engineering such as lagging variables we could obtain an improved RMSE for our MARS model, though this did not translate to the NN model. Overall, we learned the importance of (1) considering a models interpretability, (2) trade-off between improved accuracy and computational processing time, (3) assessing the relevance of our models in a dynamic world, and (4) critical thinking of what could be the root cause of issues such as overfitting and attempting to overcome some of these. These are critical factors to consider when building and implementing machine learning models in real-world scenarios. 
